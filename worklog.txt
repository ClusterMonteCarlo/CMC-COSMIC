Mar 4 2012
Bharath
---------------

I have been getting these errors at the end of the timestep, and also earlier
in random points of execution. No predictability as to when the code will fail
and throw such an error. I figured out today that this is because either
memory is accessed beyond an amount allocated. as in:

char *vnew2_arr = (char *) malloc ((clus.N_MAX_NEW+1) * sizeof(char));
for (i = 1; i <= clus.N_MAX_NEW; i++) {
	if (vnew2_arr[i] ==1) {
		.
		.
		.


OR
when an MPI function tries to communicate more than what is allocated. as in:

SplitterArray = (double*) malloc((procs-1) * sizeof(double));
MPI_Bcast(splitterArray, procs, MPI_DOUBLE, 0, MPI_COMM_WORLD);
.
.
.

Be wary of this in future.

Example of error:
******************************************************************************
tcount=4 TotalTime=1.7755249694258547e-02 Dt=6.3971496901400384e-03
Etotal=-0.25 max_r=743.622 N_bound=99997 Rtidal=999990
Mtotal=0.99997 Etotal.P=-0.499806 Etotal.K=0.249806 VRatio=0.99961
TidalMassLoss=0.00018
core_radius=0.361361 rho_core=1.14582 v_core=inf Trc=inf conc_param=2057.84
N_core=11324
trh=0 rh=0 rh_single=0 rh_binary=0
N_b=0 M_b=0 E_b=0
******************************************************************************
DIAGNOSTIC: CheckStop(): No. of timesteps > T_MAX_COUNT ... Terminating.
17.0100 seconds of processing
The total number of bisections is 0

*** glibc detected *** ../cmc_mpi: corrupted double-linked list:
0x0000000013cb77e0 ***
======= Backtrace: =========
/lib64/libc.so.6[0x342b66f38c]
/lib64/libc.so.6(cfree+0x8c)[0x342b672b1c]
.
.
.

Mar 5 2012
Bharath
---------------

There are a lot of comments I wrote throughout the code. Sit with Stefan and
fix all of them.


Mar 14 2012
Bharath
---------------

The serial version with the -s argument itself is not optimized, so the
speedup is inaccurate. Particularly for the routine findProcForIndex(), has a
loop over nprocs (emulated with -s), and sometimes is called for each star.
n this case
complexity would be N * nprocs. This function can be optimized, and also is
unnecessary in the serial version and its effect should not be considered
while measuring speedup. Need to think!
Stefan suggested using 2 loops which will simplify this, but that is a lot of
work!

Mar 16 2012
Bharath
---------------

Still some bugs that remain to be fixed include:
1) mpi_file_merge() throws weird errors when run for large procs.
2) free_arrays() -> memory free error occurs when binary array is freed.
Currently commented out.
3) gsl_rng_free() throws memory error.
Stefan's suggestion: Fix by printing out the rng's value at many places, and
see if it changes anywhere.

Apr 10 2012
Bharath
---------------

Changes: 
1)Fixed a bug - tidally_strip_stars() was called after compute_intermediate.., 
so stripped stars whose rnew values were set to INF were
not updated to r from rnew. Changed the orded in which these 2 fns are called
2)Fixed cat_and_rm_files to previous version which is apparently faster
3)Added new version of sample sort with a cleaner implementation, and a better
load balancing function that handles extremely imbalanced load which might be
due to poor sampling.

To do:
1) See if cat_rm_files can be done in parallel instead of only by root
2) Use parallel filesystem on Hopper, and may be on fugu


Apr 11 2012
Bharath
--------------
File outputs dont work. For now, the corresponding function is commented out
in print_results(). If uncommented, throws seg fault. no clue why! I tried
debugging a while today, but cant fix it. Need to be fixed later.
Also, tidalmassloss values are not right in the output, check!
A small portion of code in PrintFileOutput() has been commented out by me
since it was throwing seg. fault. needs to be fixed later.
Stefan also asked me long back to check how the new energy transfer functions,
and to make some plot to check something. Now forgot what :)
I just tested 953, and tested for 30 timesteps and 10^5 stars and the results
printed on the screen match perfectly without any differences for each
timestep till 30. Yay!!!

Apr 30 2012
Bharath
--------------

In central_calculate(), there is a loop:
Apparently the num ofcentral stars is 300. But in the parallel version, this
seems to be calculating for all the stars! Possible BUGGGG???!!

#ifdef USE_MPI
   for (i=1; i<=mpiEnd-mpiBegin+1; i++) {
#else
   for (i=1; i<=MIN(NUM_CENTRAL_STARS, clus.N_STAR); i++) {
#endif


Jun 20 2012
Bharath
--------------
Starting to work on binaries and star creation...

parser()
For now, allocating memory for binaries same as before. Might need review
later.

load_fits_file_data()
changing the serial code to read binary data from the cfd struct - ignoring
the bs_index variable. Hopefully should not create a problem with the serial
version.
serial version to be TESTED.

The binary array too seems to have a sentinel, not really, but the 0th element
is never used (as per Stefan). So, the code is appropriately written. Have to
be tested later for bugs.

What earlier was thought as not required - calc_sigma_new() resurfaced with an
error in dynamics apply, where there is a bisection in break_wide_binaries().
So, including calc_sigma_new() back.

However, the sigma value in the parallel version might not be same as the
serial version (look at comments in function header). Should be kept in mind
while doing comparison, also may be try to modify the serial version to mimic the parallel.

The calc_sigma_r fn is called once during initialization (since it might be
needed for calculating kT in timestep calculation), and once in dynamics where
it is used in break_wide_binaries(). I noticed that this fn, as well as
central_calc() were called before calc_potential, and they were using the values 
mpiBegin and mpiEnd - this was a bug., so I moved the potential calc before this fn. 
And also Start and End values were generated twice befoer and after get_star_data(), 
fixed that too.

TODO: change exit_cleanly() calls to take in argument as to which function
called exit, and print it out before calling mpi_abort(). Or alternatively
make sure there is an eprintf before every exit_cleanly() call.

TODO: Parallelize SE/Fortran rng.

newstarid, and assiging ids to newstars is taken care of. newstarid is
initialized by N_STAR+N_BINARY, otherwise, if followed the same procedure as
in sequential code, we might have to do a reduction.

Changed create_star() - didnt understand why get_global_idx() is called? :\
!!!
